# DEEP-LEARNING
I have applied the Neural Network on the MNIST dataset.

## Multi Layer Perceptron (MLP)

ABBREVATIONS <BR>
1.BN-> BATCH NORMALISATION<br>
2.DP-> DROPOUTS<br>
3.IP-> INPUT LAYER<br>
4.OP-> OUTPUT LAYER<br>

### FIRST MODEL
NO. OF HIDDEN LAYERS: 2
ARCHITECTURE OF MODEL: IP->400->BN->DP->150->BN->DP->OP
Categorical Crossentropy Loss: 0.0617 
ACCURACY: 98.14%

### SECOND MODEL
NO. OF HIDDEN LAYERS: 3
ARCHITECTURE OF MODEL: IP->600->BN->DP->300->BN->DP->250->BN->DP->OP
Categorical Crossentropy Loss: 0.0585  
ACCURACY: 98.37%

### THIRD MODEL
NO. OF HIDDEN LAYERS: 5
ARCHITECTURE OF MODEL: IP->600->BN->DP->500->BN->DP->400->BN->DP->300->BN->DP->250->BN->DP->OP
Categorical Crossentropy Loss: 0.0601  
ACCURACY: 98.32%

### FOURTH MODEL
NO. OF HIDDEN LAYERS: 5 ONLY WITH DROPOUTS
ARCHITECTURE OF MODEL: IP->700->DP->550->DP->390->DP->290->DP->670->DP->OP
Categorical Crossentropy Loss: 0.6135  
ACCURACY: 74.38%

### FIFTH MODEL
NO. OF HIDDEN LAYERS: 4 WITH ALTERNATIVE DROPOUTS
ARCHITECTURE OF MODEL: IP->790->390->DP->290->670->DP->OP
Categorical Crossentropy Loss: 0.469  
ACCURACY: 82.42%









