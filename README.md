# DEEP-LEARNING
I have applied the Neural Network on the MNIST dataset.

## Multi Layer Perceptron (MLP)

ABBREVATIONS <BR>
1.BN-> BATCH NORMALISATION<br>
2.DP-> DROPOUTS<br>
3.IP-> INPUT LAYER<br>
4.OP-> OUTPUT LAYER<br>

### FIRST MODEL
NO. OF HIDDEN LAYERS: 2<br>
ARCHITECTURE OF MODEL: IP->400->BN->DP->150->BN->DP->OP<br>
Categorical Crossentropy Loss: 0.0617 <br>
ACCURACY: 98.14%<br>

### SECOND MODEL
NO. OF HIDDEN LAYERS: 3<br>
ARCHITECTURE OF MODEL: IP->600->BN->DP->300->BN->DP->250->BN->DP->OP<br>
Categorical Crossentropy Loss: 0.0585  <br>
ACCURACY: 98.37%<br>

### THIRD MODEL
NO. OF HIDDEN LAYERS: 5<br>
ARCHITECTURE OF MODEL: IP->600->BN->DP->500->BN->DP->400->BN->DP->300->BN->DP->250->BN->DP->OP<br>
Categorical Crossentropy Loss: 0.0601  <br>
ACCURACY: 98.32%<br>

### FOURTH MODEL
NO. OF HIDDEN LAYERS: 5 ONLY WITH DROPOUTS<br>
ARCHITECTURE OF MODEL: IP->700->DP->550->DP->390->DP->290->DP->670->DP->OP<br>
Categorical Crossentropy Loss: 0.6135  <br>
ACCURACY: 74.38%<br>

### FIFTH MODEL
NO. OF HIDDEN LAYERS: 4 WITH ALTERNATIVE DROPOUTS<br>
ARCHITECTURE OF MODEL: IP->790->390->DP->290->670->DP->OP<br>
Categorical Crossentropy Loss: 0.469  <br>
ACCURACY: 82.42%<br>









